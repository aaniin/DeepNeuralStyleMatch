{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Style Match\n",
    "\n",
    "Use definition of neural style by Gatys et al. (2015) (https://arxiv.org/abs/1508.06576) and match an unknown picture to an artist's style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from nst_utils import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load VGG\n",
    "\n",
    "We'll use VGG-19 (arXiv:1409.1556) trained on ImageNet. Download from http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\n",
    "The load_vgg_model function has some copyright. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Visualize paintings of a few artists\n",
    "\n",
    "**Caveat: Need to download some images from the web to make this work** \n",
    "\n",
    "Visualize some data. We have images 1 to 6 by chagall, kinkade, matisse, monet, and picasso. \n",
    "\n",
    "Images are randomnly selected, execute several times to see the full database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#content_image = scipy.misc.imread(\"images/louvre.jpg\")\n",
    "artists = [\"chagall\", \"kinkade\", \"matisse\", \"monet\", \"picasso\"]\n",
    "artist = random.choice(artists)\n",
    "print(artist)\n",
    "image_name = artist+str('-')+str(random.randint(1,6))\n",
    "print(image_name)\n",
    "example_image = imageio.imread('artist-styles/'+image_name+'.jpg')\n",
    "imshow(example_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Computing the style cost\n",
    "\n",
    "Create a function which takes a picture and calculates its style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 - Style matrix\n",
    "\n",
    "The neural style paper by Gatys et al. defines style as the gram matrix of activations. The gram matrix is the inner product (dot product) of the activations ${\\displaystyle G_{ij} = \\langle v_i,v_j\\rangle = v_{i}^T v_{j} = np.dot(v_{i}, v_{j})  }$ see https://en.wikipedia.org/wiki/Gramian_matrix. \n",
    "\n",
    "We use the unrolled activations (i.e. 300 x 300 pixel-activation-matrix -> 90000 element vector) as the $v_i$ and $v_j$. Say one filter detects edges and one filter detects green. The strenght of the unnormalized correlation (unnormalized correlation $C_{ij} = v_{i} \\cdot v_{j}$, not correlation coefficient $c_{ij} = \\frac{v_{i} \\cdot v_{j}}{|v_i|\\cdot|v_j|}$) will tell you that whenever there's an edge, it's green! You just found the artist likes green edges. Importantly, the diagonal elements of the gram matrix measure how strongly a feature is prevalent. Say a filter detects horizontal lines, and there are a lot of horizontal lines in the picture, the gram matrix will show!\n",
    "As Andrew puts it: \n",
    "> By capturing the prevalence of different types of features ($G_{ii}$), as well as how much different features occur together ($G_{ij}$), the Style matrix $G$ measures the style of an image. \n",
    "\n",
    "The gram matrix of A is $G_A = AA^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram Matrix\n",
    "def gram_matrix(A):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    A -- matrix of shape (n_C, n_H*n_W)\n",
    "    \n",
    "    Returns:\n",
    "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    GA = tf.tensordot(A,tf.transpose(A),axes=1) # axes is over how many axes should be summed\n",
    "    #GA = tf.matmul(A,A,transpose_a=False,transpose_b=True) #also works\n",
    "    \n",
    "    return GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gram matrix fuct\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    A = [[1,2],[3,4]]\n",
    "    GA = gram_matrix(A)\n",
    "    \n",
    "    print(\"GA = \" + str(GA.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            GA = \n",
    "        </td>\n",
    "        <td>\n",
    "        [[ 5 11]\n",
    "         [11 25]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Image style - helper functions\n",
    "So far, we can calcluate the gram matrix (i.e. style) for a given activation of a particular layer. Let's define some helper functions to calculate the style of an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_style(a_S):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n",
    "    Returns: \n",
    "    G_S -- Gram matrix of layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from a_S\n",
    "    #m, n_H, n_W, n_C = a_S.shape # if a_S was an ndarray\n",
    "    m, n_H, n_W, n_C = a_S.get_shape().as_list()\n",
    "    \n",
    "    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)\n",
    "    a_S = tf.transpose(tf.reshape(a_S,[n_H*n_W,n_C]))\n",
    "\n",
    "    # Computing gram_matrices for both images S and G (≈2 lines)\n",
    "    G_S = gram_matrix(a_S)\n",
    "    \n",
    "    return G_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test layer_style\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    # some arbitrary activation of shape (m=1,n_H,n_W,n_C)\n",
    "    tf.set_random_seed(1)\n",
    "    a_S = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    # the gram matrix \n",
    "    my_style = layer_style(a_S)\n",
    "    \n",
    "    print(\"my_style = \" + str(my_style.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: What is the expected result of above cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The layers for which we want to extract their style\n",
    "# and the weigth (used later) for calculating the style cost\n",
    "STYLE_LAYERS = [\n",
    "    ('conv1_1', 0.2),\n",
    "    ('conv2_1', 0.2),\n",
    "    ('conv3_1', 0.2),\n",
    "    ('conv4_1', 0.2),\n",
    "    ('conv5_1', 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style(tf_session, model, STYLE_LAYERS):\n",
    "    \n",
    "    GS = [] # empty array\n",
    "    # Loop over specified layers\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "        #print(layer_name)\n",
    "        \n",
    "        # Select this hidden layer's activation\n",
    "        a_S = model[layer_name]\n",
    "    \n",
    "        # Get the gram matrix for this layer\n",
    "        GS_i = layer_style(a_S)\n",
    "        # GS_i is of shape (n_C,n_C)\n",
    "        # print(GS_i.shape)\n",
    "        \n",
    "        GS_i = tf_session.run(GS_i)\n",
    "        assert (isinstance(GS_i,np.ndarray))\n",
    "        GS.append(GS_i)\n",
    "\n",
    "    return GS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to calculate an activation in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "\n",
    "    style_image = reshape_and_normalize_image(example_image)\n",
    "\n",
    "    # Assign the content image to be the input of the VGG model.  \n",
    "    sess.run(model['input'].assign(style_image))\n",
    "\n",
    "    # Select the output tensor of layer conv4_2\n",
    "    out = model['conv4_2']\n",
    "\n",
    "    # Set a_C to be the hidden layer activation from the layer we have selected\n",
    "    a_C = sess.run(out)\n",
    "\n",
    "    print(a_C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Style of an image\n",
    "Let's calculate the style of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "\n",
    "    style_image = reshape_and_normalize_image(example_image)\n",
    "\n",
    "    # Assign the content image to be the input of the VGG model.  \n",
    "    sess.run(model['input'].assign(style_image))\n",
    "\n",
    "    style = compute_style(sess, model,STYLE_LAYERS)\n",
    "    \n",
    "    # style is a list of nd arrays\n",
    "#     for i in style:\n",
    "#         print(type(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 - Style database\n",
    "We can capture the style of an image, so let's build a database of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_db = dict()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "\n",
    "#\n",
    "# reference styles\n",
    "#\n",
    "# artists defined above as artists = [\"chagall\", \"kinkade\", \"matisse\", \"monet\", \"picasso\"]\n",
    "for artist in artists:\n",
    "    print(artist)\n",
    "    for i in range(1,6):\n",
    "            \n",
    "        with tf.Session() as sess:\n",
    "                \n",
    "            image_name = artist+str('-')+str(i)\n",
    "            print(image_name)\n",
    "            style_image = imageio.imread('artist-styles/'+image_name+'.jpg')\n",
    "            style_image = reshape_and_normalize_image(style_image)\n",
    "\n",
    "            # Assign the content image to be the input of the VGG model.  \n",
    "            sess.run(model['input'].assign(style_image))\n",
    "\n",
    "            style = compute_style(sess, model,STYLE_LAYERS)\n",
    "#             print(type(style))\n",
    "#             for i in style:\n",
    "#                 print(type(i))\n",
    "\n",
    "            style_db[image_name] = style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 - Target style\n",
    "Having calculated the style of a bunch of paintings, let's calculate the style of the target picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_style = list()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "\n",
    "    #\n",
    "    # target style \n",
    "    #\n",
    "    image_name = 'target'\n",
    "    print(image_name)\n",
    "    style_image = imageio.imread('artist-styles/'+image_name+'.jpg')\n",
    "    style_image = reshape_and_normalize_image(style_image)\n",
    "\n",
    "    # Assign the content image to be the input of the VGG model.  \n",
    "    sess.run(model['input'].assign(style_image))\n",
    "\n",
    "    target_style = compute_style(sess,model,STYLE_LAYERS)\n",
    "    print(type(target_style))\n",
    "    for i in target_style:\n",
    "        print(type(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style distance\n",
    "We calculated the style for our target picture $T$ and a variety of reference paintings $R$. Now we find the picture in the db with the minimal distance to our target style. We use the difference in Gram matrix $G$ For each layer, the distance is\n",
    "\n",
    "$$dist^{[l]}(R,T) = \\frac{1}{4 \\times {n_C}^2 \\times (n_H \\times n_W)^2} \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(R)}_{ij} - G^{(T)}_{ij})^2\\tag{2} $$\n",
    "\n",
    "where $G^{(R)}$ and $G^{(T)}$ are respectively the Gram matrices of the \"reference\" db image and the \"target\" image, computed using the hidden layer activations for a particular hidden layer in the network.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nc_nh_nw_list(model,STYLE_LAYERS):\n",
    "    \n",
    "    nc_nh_nw_list = list()\n",
    "    # loop over the relevant layers\n",
    "    for layer_name, coeff in STYLE_LAYERS:\n",
    "        print(layer_name)\n",
    "\n",
    "        # Set a_S to be the hidden layer activation from the layer we have selected\n",
    "        a_S = sess.run(model[layer_name])\n",
    "        \n",
    "        m, n_H, n_W, n_C = a_S.shape\n",
    "        nc_nh_nw_list.append([n_C, n_H, n_W])\n",
    "        \n",
    "        print(n_C, n_H, n_W)\n",
    "        #print(list)\n",
    "    return nc_nh_nw_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_nh_nw_list = list()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = load_vgg_model(\"pretrained-model/imagenet-vgg-verydeep-19.mat\")\n",
    "\n",
    "    # Our target image\n",
    "    image_name = 'target'\n",
    "    print(image_name)\n",
    "    style_image = imageio.imread('artist-styles/'+image_name+'.jpg')\n",
    "    style_image = reshape_and_normalize_image(style_image)\n",
    "\n",
    "    # Assign the content image to be the input of the VGG model.  \n",
    "    sess.run(model['input'].assign(style_image))\n",
    "    \n",
    "    nc_nh_nw_list = get_nc_nh_nw_list(model,STYLE_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(ref_list,tar_list,nc_nh_nw_list):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ref_list -- list of gram matrices of reference picture\n",
    "    tar_list -- list of gram matrices of target picture\n",
    "    nc_nh_nw_list -- list of nc = number of channels, nh = number of pixels in height, nw = width\n",
    "    \"\"\"\n",
    "    \n",
    "    dist = 0\n",
    "    for (ref_G_L, tar_G_L,(n_C,n_H,n_W)) in zip(ref_list,tar_list,nc_nh_nw_list):\n",
    "#         print(ref_G_L.shape)\n",
    "#         print(tar_G_L.shape)\n",
    "#         print(n_C,n_H,n_W)\n",
    "#         print('-------')\n",
    "        #print(type(ref_G_L))\n",
    "        dist += 1/(2*n_C*n_H*n_W)**2*np.sum(np.square(np.subtract(ref_G_L,tar_G_L)))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = 10**100\n",
    "min_name = \"None\"\n",
    "for key, value in style_db.items():\n",
    "    dist = get_dist(value,target_style,nc_nh_nw_list)\n",
    "    print(\"min_dist {}, dist {}, key {}\".format(min_dist,dist,key))\n",
    "    if (dist < min_dist):\n",
    "        min_dist = dist\n",
    "        min_name = key\n",
    "print(min_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_dict = dict()\n",
    "for key, value in style_db.items():\n",
    "    dist = get_dist(value,target_style,nc_nh_nw_list)\n",
    "    dist_dict[key] = dist\n",
    "sorted(dist_dict, key=dist_dict.__getitem__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "We see that the target picture's distance to the kinkades is small. This means the target picture is predicted to be a kinkade by the algorithm. In fact, the target picture is indeed one of Kinkade's paintings. \n",
    "\n",
    "**Shortcomings**:\n",
    "In one shot learning, we would train to optimize for a large difference between invalid pairs (say picasso - monet) and a small difference between valid pairs (chagall - chagall). We don't put in the effort of actually doing this since the implementation already has 100% accuracy on our small test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "owWbQ",
   "launcher_item_id": "lEthw"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
